# SuperAI Platform - vLLM Inference Engine StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: vllm
  namespace: superai
  labels:
    app.kubernetes.io/name: vllm
    app.kubernetes.io/component: inference-engine
    app.kubernetes.io/part-of: superai
spec:
  serviceName: vllm-svc
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: vllm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vllm
        app.kubernetes.io/component: inference-engine
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8001"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: superai-sa
      terminationGracePeriodSeconds: 120
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.6.6
          ports:
            - name: http
              containerPort: 8001
          args:
            - "--host=0.0.0.0"
            - "--port=8001"
            - "--model=meta-llama/Llama-3.1-8B-Instruct"
            - "--gpu-memory-utilization=0.90"
            - "--max-model-len=32768"
            - "--enable-prefix-caching"
            - "--enable-chunked-prefill"
            - "--disable-log-requests"
            - "--served-model-name=llama-3.1-8b-instruct"
          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: superai-secrets
                  key: HF_TOKEN
            - name: VLLM_ATTENTION_BACKEND
              value: "FLASH_ATTN"
          resources:
            requests:
              cpu: "4"
              memory: "16Gi"
              nvidia.com/gpu: "1"
            limits:
              cpu: "8"
              memory: "32Gi"
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: shm
              mountPath: /dev/shm
          readinessProbe:
            httpGet:
              path: /health
              port: 8001
            initialDelaySeconds: 120
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 8001
            initialDelaySeconds: 180
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 5
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
  volumeClaimTemplates:
    - metadata:
        name: model-cache
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: fast-ssd
        resources:
          requests:
            storage: 100Gi
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-svc
  namespace: superai
  labels:
    app.kubernetes.io/name: vllm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8001
      targetPort: 8001
  selector:
    app.kubernetes.io/name: vllm